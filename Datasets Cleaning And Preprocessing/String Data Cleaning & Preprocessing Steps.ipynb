{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05513be1",
   "metadata": {},
   "source": [
    "# String Data Cleaning & Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90e0ff",
   "metadata": {},
   "source": [
    "Here we use Python and regular expressions to perform some basic text cleaning and preprocessing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce915fd9",
   "metadata": {},
   "source": [
    "\n",
    "# 1.Cleaning of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0301a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98e5b2",
   "metadata": {},
   "source": [
    "The re.sub() function in Python is used to replace all occurrences of a pattern in a string with a replacement string. The first argument is the pattern to be matched, and the second argument is the replacement string. The third argument is the string that the function will operate on.\n",
    "\n",
    "In the specific example you provided, re.sub(r'[^\\w\\s]','',text) is used to remove all characters that are not alphanumeric or whitespace.\n",
    "\n",
    "The regular expression [^\\w\\s] matches any character that is not a word character (\\w) or a whitespace character (\\s).\n",
    "\n",
    "\\w matches any alphanumeric character (letter, digit, or underscore)\n",
    "\\s matches any whitespace character (space, tab, newline, etc.)\n",
    "The ^ inside the square brackets negates the set, so it matches any character that is not in the set.\n",
    "\n",
    "The '' is the replacement string, which means that the matched characters will be removed, and replaced by an empty string, resulting in removing all non-alphanumeric and non-whitespace characters from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d9df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='     my # Name is !@#$^&*+,Abdul Remhan  2324325    '\n",
    "\n",
    "text=re.sub(r'[^\\w\\s]','',text)  #Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a96f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     my  Name is Abdul Remhan  2324325    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bffbf65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     my  name is abdul remhan  2324325    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=text.lower()   #convert to lower case\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d6365",
   "metadata": {},
   "source": [
    "The re.sub(r'\\d+', '', text) function is used to remove all digits from the string.\n",
    "\n",
    "\\d is a special character that matches any digit, and the + after it indicates that one or more consecutive digits should be matched. So, \\d+ matches one or more consecutive digits in the string.\n",
    "\n",
    "The re.sub() function then rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2518d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     my  name is abdul remhan      '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=re.sub(r'\\d+','',text)  #Remove the digits\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55781d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my  name is abdul remhan'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=text.strip()   #Removing Whitespaces from text\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda2df5",
   "metadata": {},
   "source": [
    "### After All of Explainantion,I can make the combine fnction for Text cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f808c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(txt):\n",
    "    txt=re.sub(r'[^\\w\\s]','',txt)\n",
    "    txt=txt.lower()\n",
    "    txt=re.sub(r'\\d+','',txt)\n",
    "    txt=txt.strip()\n",
    "    \n",
    "    return(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39215fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is abdul rehman i am in class  my email is abdul rehmangmailcom  i am  years old'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_text('     My name is Abdul Rehman .I am In class 9 .My Email is: Abdul .rehman@gmail.com  I am 21 years old    ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ba11f",
   "metadata": {},
   "source": [
    "# 2.Tokenization of Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4ad30",
   "metadata": {},
   "source": [
    "Tokenization is a critical step in natural language processing as it allows you to work with individual words and sentences, which can be useful for tasks such as text classification, information retrieval, and text generation. In addition, it allows for the removal of stopwords and stemming/lemmatization of the words, which in turn can improve the results of the NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e51c40",
   "metadata": {},
   "source": [
    "nltk (Natural Language Toolkit) is a python library that provides tools to work with human language data, such as text. It provides a wide range of functionality for tasks such as tokenization, stemming, lemmatization, parsing, semantic reasoning, and wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "Punkt tokenizer is trained on a large corpus of text and is able to accurately tokenize text in multiple languages, including English, German, and Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91e01c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258b038",
   "metadata": {},
   "source": [
    "word_tokenize() is also a function provided by the nltk.tokenize module, which is used to tokenize a piece of text into words. It takes a string of text as input and returns a list of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9d1042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'abdul',\n",
       " 'rehman',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'class',\n",
       " 'my',\n",
       " 'email',\n",
       " 'is',\n",
       " 'abdul',\n",
       " 'rehmangmailcom',\n",
       " 'i',\n",
       " 'am',\n",
       " 'years',\n",
       " 'old']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  #laibrary for word tokenization \n",
    "some_txt='my name is abdul rehman i am in class  my email is abdul rehmangmailcom  i am  years old'\n",
    "word_tokens=word_tokenize(some_txt)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c621140",
   "metadata": {},
   "source": [
    "sent_tokenize() is a function provided by the nltk.tokenize module, which is used to tokenize a piece of text into sentences. It takes a string of text as input and returns a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ccc537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my name is abdul rehman i am in class  my email is abdul rehmangmailcom  i am  years old']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize  #laibrary for sentences tokenization \n",
    "some_txt='my name is abdul rehman i am in class  my email is abdul rehmangmailcom  i am  years old'\n",
    "sent_tokens=sent_tokenize(some_txt)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97c56b",
   "metadata": {},
   "source": [
    "Both sent_tokenize() and word_tokenize() use Punkt tokenizer, a pre-trained unsupervised machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b32e69",
   "metadata": {},
   "source": [
    "# 3.Removing StopWords From Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2bb9c",
   "metadata": {},
   "source": [
    "Stop words are common words that are typically removed from text data before performing natural language processing tasks such as text classification or text generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b1a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  #importing stopwords laibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e71f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  #download stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dc3535c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words=list(stopwords.words('english'))  #list all the english stopwords\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e5bfc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'abdul',\n",
       " 'rehman',\n",
       " 'class',\n",
       " 'email',\n",
       " 'abdul',\n",
       " 'rehmangmailcom',\n",
       " 'years',\n",
       " 'old']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list to seperate the words which are not stop words\n",
    "\n",
    "filtered_words=[word for word in word_tokens if word not in stop_words] \n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592c505",
   "metadata": {},
   "source": [
    "# 3. Conversion of words in to their Base Roots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b23f3",
   "metadata": {},
   "source": [
    "###  Stemming or limitization of the Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7454d16",
   "metadata": {},
   "source": [
    "Stemming or limitization of the word is the process of reducing words to their base or root form. This can be useful for tasks such as text classification or information retrieval, as it can help to group together words that have the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6bc634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  #importing stemming laibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2919383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'abdul',\n",
       " 'rehman',\n",
       " 'class',\n",
       " 'email',\n",
       " 'abdul',\n",
       " 'rehmangmailcom',\n",
       " 'year',\n",
       " 'old']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemer=PorterStemmer()\n",
    "stemed_words=[stemer.stem(word)for word in filtered_words] #making the list of the stemed words\n",
    "stemed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfea265",
   "metadata": {},
   "source": [
    "Limitization has the same functionality as stemming but different function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb6927",
   "metadata": {},
   "source": [
    "# 4. Encoding of Tokenized Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b96ffb",
   "metadata": {},
   "source": [
    "There are two fuctions which are used to apply an encoder on tokenized text, you can use a method such as texts_to_matrix() or texts_to_sequences() from the Tokenizer class.Both methods are same in working and approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c229d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\user\\anaconda3\\lib\\site-packages (2.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066d6e9",
   "metadata": {},
   "source": [
    "### Difference Between texts_to_matrix function and text_to_sequence function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad953f6",
   "metadata": {},
   "source": [
    "In short, texts_to_matrix() converts a list of texts into a bag-of-words representation, while texts_to_sequences() converts a list of texts into a list of lists of integers, where each integer corresponds to a word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c908717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.21)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.30.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f319276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab0d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It converts the data into binary numerical form which is acceptable for machine learning\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "958820b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.texts_to_matrix(stemed_words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11b3d63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191db74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
